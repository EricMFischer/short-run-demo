{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "short_run_demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSJrJXeHWpDVhj9nRogqqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricMFischer/short_run_demo/blob/master/short_run_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wmSCx3X_saP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "53ac0e1e-5a50-4d8b-9ad0-27d861698f86"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8AYEnYXAJww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fafb8378-269d-4544-a64b-cccecf5328bf"
      },
      "source": [
        "%cd gdrive/My Drive/short_run_demo"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/short_run_demo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdZt9NyYAYf5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "8ebd9181-34b7-4242-bb43-f91f03942787"
      },
      "source": [
        "! git clone https://github.com/EricMFischer/short_run_demo.git\n",
        "# To pull from the repository at any point, run '! git pull'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'short_run_demo'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 22 (delta 10), reused 22 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQl_H-VJBELh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install torch==1.3.0\n",
        "# ! pip install torchvision==0.4.0\n",
        "# ! pip install matplotlib\n",
        "# ! pip install numpy\n",
        "# ! pip install scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sary_pdBMap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import *\n",
        "from nets import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csc_eVteBwvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############################################\n",
        "# ## TRAIN EBM USING 2D TOY DISTRIBUTION ## #\n",
        "#############################################\n",
        "# Measures and plots the diagnostic values d_{s_t} and r_t\n",
        "\n",
        "'''\n",
        "GOAL: Implement another toy density for learning 2D DeepFRAME models.\n",
        "Modify ToyDataset class in utils to implement another density.\n",
        "Density should be a GMM groundtruth density with several modes that have\n",
        "different covariance matrices.\n",
        "This file (train_toy.py) should run without changes once new dataset class has\n",
        "been made.\n",
        "'''\n",
        "\n",
        "import torch as t\n",
        "import json\n",
        "import os\n",
        "from nets import ToyNet\n",
        "from utils import plot_diagnostics, ToyDataset\n",
        "\n",
        "# directory for experiment results\n",
        "EXP_DIR = './out_toy/toy_config_1/'\n",
        "# json file with experiment config\n",
        "CONFIG_FILE = './config_locker/toy_config.json'\n",
        "\n",
        "\n",
        "#######################\n",
        "# ## INITIAL SETUP ## #\n",
        "#######################\n",
        "\n",
        "# load experiment config\n",
        "with open(CONFIG_FILE) as file:\n",
        "    config = json.load(file)\n",
        "\n",
        "# make directory for saving results\n",
        "if os.path.exists(EXP_DIR):\n",
        "    # prevents overwriting old experiment folders by accident\n",
        "    raise RuntimeError('Experiment folder \"{}\" already exists. Please use a different \"EXP_DIR\".'.format(EXP_DIR))\n",
        "else:\n",
        "    os.makedirs(EXP_DIR)\n",
        "    for folder in ['checkpoints', 'landscape', 'plots', 'code']:\n",
        "        os.mkdir(EXP_DIR + folder)\n",
        "\n",
        "# save copy of code in the experiment folder\n",
        "def save_code():\n",
        "    def save_file(file_name):\n",
        "        file_in = open('./' + file_name, 'r')\n",
        "        file_out = open(EXP_DIR + 'code/' + os.path.basename(file_name), 'w')\n",
        "        for line in file_in:\n",
        "            file_out.write(line)\n",
        "    for file in ['train_toy.py', 'nets.py', 'utils.py', CONFIG_FILE]:\n",
        "        save_file(file)\n",
        "save_code()\n",
        "\n",
        "# set seed for cpu and CUDA, get device\n",
        "t.manual_seed(config['seed'])\n",
        "if t.cuda.is_available():\n",
        "    t.cuda.manual_seed_all(config['seed'])\n",
        "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "########################\n",
        "# ## TRAINING SETUP # ##\n",
        "########################\n",
        "\n",
        "print('Setting up network and optimizer...')\n",
        "# set up network\n",
        "net_bank = {'toy': ToyNet}\n",
        "f = net_bank[config['net_type']]().to(device)\n",
        "# set up optimizer\n",
        "optim_bank = {'adam': t.optim.Adam, 'sgd': t.optim.SGD}\n",
        "if config['optimizer_type'] == 'sgd' and config['epsilon'] > 0:\n",
        "    # scale learning rate according to langevin noise for invariant tuning\n",
        "    config['lr_init'] *= (config['epsilon'] ** 2) / 2\n",
        "    config['lr_min'] *= (config['epsilon'] ** 2) / 2\n",
        "optim = optim_bank[config['optimizer_type']](f.parameters(), lr=config['lr_init'])\n",
        "\n",
        "print('Processing data...')\n",
        "# toy dataset for which true samples can be obtained\n",
        "q = ToyDataset(config['toy_type'], config['toy_groups'], config['toy_sd'],\n",
        "               config['toy_radius'], config['viz_res'], config['kde_bw'])\n",
        "\n",
        "# initialize persistent states from noise \n",
        "# s_t_0 is used when init_type == 'persistent' in sample_s_t()\n",
        "s_t_0 = 2 * t.rand([config['s_t_0_size'], 2, 1, 1]).to(device) - 1\n",
        "\n",
        "\n",
        "################################\n",
        "# ## FUNCTIONS FOR SAMPLING ## #\n",
        "################################\n",
        "\n",
        "# sample batch from given array of states\n",
        "def sample_state_set(state_set, batch_size=config['batch_size']):\n",
        "    rand_inds = t.randperm(state_set.shape[0])[0:batch_size]\n",
        "    return state_set[rand_inds], rand_inds\n",
        "\n",
        "# sample positive states from toy 2d distribution q\n",
        "def sample_q(batch_size=config['batch_size']): return t.Tensor(q.sample_toy_data(batch_size)).to(device)\n",
        "\n",
        "# initialize and update states with langevin dynamics to obtain samples from finite-step MCMC distribution s_t\n",
        "def sample_s_t(batch_size, L=config['num_mcmc_steps'], init_type=config['init_type'], update_s_t_0=True):\n",
        "    # get initial mcmc states for langevin updates (\"persistent\", \"data\", \"uniform\", or \"gaussian\")\n",
        "    def sample_s_t_0():\n",
        "        if init_type == 'persistent':\n",
        "            return sample_state_set(s_t_0, batch_size)\n",
        "        elif init_type == 'data':\n",
        "            return sample_q(batch_size), None\n",
        "        elif init_type == 'uniform':\n",
        "            return config['noise_init_factor'] * (2 * t.rand([batch_size, 2, 1, 1]) - 1).to(device), None\n",
        "        elif init_type == 'gaussian':\n",
        "            return config['noise_init_factor'] * t.randn([batch_size, 2, 1, 1]).to(device), None\n",
        "        else:\n",
        "            raise RuntimeError('Invalid method for \"init_type\" (use \"persistent\", \"data\", \"uniform\", or \"gaussian\")')\n",
        "\n",
        "    # initialize MCMC samples\n",
        "    x_s_t_0, s_t_0_inds = sample_s_t_0()\n",
        "\n",
        "    # iterative langevin updates of MCMC samples\n",
        "    x_s_t = t.autograd.Variable(x_s_t_0.clone(), requires_grad=True)\n",
        "    r_s_t = t.zeros(1).to(device)  # variable r_s_t (Section 3.2) to record average gradient magnitude\n",
        "    for ell in range(L):\n",
        "        f_prime = t.autograd.grad(f(x_s_t).sum(), [x_s_t])[0]\n",
        "        x_s_t.data += - f_prime + config['epsilon'] * t.randn_like(x_s_t)\n",
        "        r_s_t += f_prime.view(f_prime.shape[0], -1).norm(dim=1).mean()\n",
        "\n",
        "    if init_type == 'persistent' and update_s_t_0:\n",
        "        # update persistent state bank\n",
        "        s_t_0.data[s_t_0_inds] = x_s_t.detach().data.clone()\n",
        "\n",
        "    return x_s_t.detach(), r_s_t.squeeze() / L\n",
        "\n",
        "\n",
        "#######################\n",
        "# ## TRAINING LOOP ## #\n",
        "#######################\n",
        "\n",
        "# containers for diagnostic records (see Section 3)\n",
        "d_s_t_record = t.zeros(config['num_train_iters']).to(device)  # energy difference between positive and negative samples\n",
        "r_s_t_record = t.zeros(config['num_train_iters']).to(device)  # average state gradient magnitude along Langevin path\n",
        "\n",
        "print('Training has started.')\n",
        "for i in range(config['num_train_iters']):\n",
        "    # obtain positive and negative samples\n",
        "    x_q = sample_q()\n",
        "    x_s_t, r_s_t = sample_s_t(batch_size=config['batch_size'])\n",
        "\n",
        "    # calculate ML computational loss d_s_t (Section 3) for data and shortrun samples\n",
        "    d_s_t = f(x_q).mean() - f(x_s_t).mean()\n",
        "    if config['epsilon'] > 0:\n",
        "        # scale loss with the langevin implementation\n",
        "        d_s_t *= 2 / (config['epsilon'] ** 2)\n",
        "    # stochastic gradient ML update for model weights\n",
        "    optim.zero_grad()\n",
        "    d_s_t.backward()\n",
        "    optim.step()\n",
        "\n",
        "    # record diagnostics\n",
        "    d_s_t_record[i] = d_s_t.detach().data\n",
        "    r_s_t_record[i] = r_s_t\n",
        "\n",
        "    # anneal learning rate\n",
        "    for lr_gp in optim.param_groups:\n",
        "        lr_gp['lr'] = max(config['lr_min'], lr_gp['lr'] * config['lr_decay'])\n",
        "\n",
        "    # print and save learning info\n",
        "    if (i + 1) == 1 or (i + 1) % config['log_info_freq'] == 0:\n",
        "        print('{:>6d}   d_s_t={:>14.9f}   r_s_t={:>14.9f}'.format(i+1, d_s_t.detach().data, r_s_t))\n",
        "        # save network weights\n",
        "        t.save(f.state_dict(), EXP_DIR + 'checkpoints/' + 'net_{:>06d}.pth'.format(i+1))\n",
        "        # plot diagnostics for energy difference d_s_t and gradient magnitude r_t\n",
        "        if (i + 1) > 1:\n",
        "            plot_diagnostics(i, d_s_t_record, r_s_t_record, EXP_DIR + 'plots/')\n",
        "\n",
        "    # visualize density and log-density for groundtruth, learned energy, and short-run distributions\n",
        "    if (i + 1) % config['log_viz_freq'] == 0:\n",
        "        print('{:>6}   Visualizing true density, learned density, and short-run KDE.'.format(i+1))\n",
        "        x_kde = sample_s_t(batch_size=config['batch_size_kde'], update_s_t_0=False)[0]\n",
        "        q.plot_toy_density(True, f, config['epsilon'], x_kde, EXP_DIR+'landscape/'+'toy_viz_{:>06d}.pdf'.format(i+1))\n",
        "        print('{:>6}   Visualizations saved.'.format(i + 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}