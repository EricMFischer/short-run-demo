{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "short_run_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNV68qijk8CyVUW6LJqswT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My Drive/short_run_demo_folder/short_run_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! git clone https://github.com/EricMFischer/short_run_demo.git\n",
        "# ! git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# utils.py\n",
        "\n",
        "# download plotting functions and toy dataset\n",
        "\n",
        "import torch as t\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "##################\n",
        "# ## PLOTTING ## #\n",
        "##################\n",
        "\n",
        "# plot diagnostics for learning\n",
        "def plot_diagnostics(batch, en_diffs, grad_mags, exp_dir, fontsize=10):\n",
        "    # axis tick size\n",
        "    matplotlib.rc('xtick', labelsize=6)\n",
        "    matplotlib.rc('ytick', labelsize=6)\n",
        "    fig = plt.figure()\n",
        "\n",
        "    def plot_en_diff_and_grad_mag():\n",
        "        # energy difference\n",
        "        ax = fig.add_subplot(221)\n",
        "        ax.plot(en_diffs[0:(batch+1)].data.cpu().numpy())\n",
        "        ax.axhline(y=0, ls='--', c='k')\n",
        "        ax.set_title('Energy Difference', fontsize=fontsize)\n",
        "        ax.set_xlabel('batch', fontsize=fontsize)\n",
        "        ax.set_ylabel('$d_{s_t}$', fontsize=fontsize)\n",
        "        # mean langevin gradient\n",
        "        ax = fig.add_subplot(222)\n",
        "        ax.plot(grad_mags[0:(batch+1)].data.cpu().numpy())\n",
        "        ax.set_title('Average Langevin Gradient Magnitude', fontsize=fontsize)\n",
        "        ax.set_xlabel('batch', fontsize=fontsize)\n",
        "        ax.set_ylabel('$r_{s_t}$', fontsize=fontsize)\n",
        "\n",
        "    def plot_crosscorr_and_autocorr(t_gap_max=2000, max_lag=15, b_w=0.35):\n",
        "        t_init = max(0, batch + 1 - t_gap_max)\n",
        "        t_end = batch + 1\n",
        "        t_gap = t_end - t_init\n",
        "        max_lag = min(max_lag, t_gap - 1)\n",
        "        # rescale energy diffs to unit mean square but leave uncentered\n",
        "        en_rescale = en_diffs[t_init:t_end] / t.sqrt(t.sum(en_diffs[t_init:t_end] * en_diffs[t_init:t_end])/(t_gap-1))\n",
        "        # normalize gradient magnitudes\n",
        "        grad_rescale = (grad_mags[t_init:t_end]-t.mean(grad_mags[t_init:t_end]))/t.std(grad_mags[t_init:t_end])\n",
        "        # cross-correlation and auto-correlations\n",
        "        cross_corr = np.correlate(en_rescale.cpu().numpy(), grad_rescale.cpu().numpy(), 'full') / (t_gap - 1)\n",
        "        en_acorr = np.correlate(en_rescale.cpu().numpy(), en_rescale.cpu().numpy(), 'full') / (t_gap - 1)\n",
        "        grad_acorr = np.correlate(grad_rescale.cpu().numpy(), grad_rescale.cpu().numpy(), 'full') / (t_gap - 1)\n",
        "        # x values and indices for plotting\n",
        "        x_corr = np.linspace(-max_lag, max_lag, 2 * max_lag + 1)\n",
        "        x_acorr = np.linspace(0, max_lag, max_lag + 1)\n",
        "        t_0_corr = int((len(cross_corr) - 1) / 2 - max_lag)\n",
        "        t_0_acorr = int((len(cross_corr) - 1) / 2)\n",
        "\n",
        "        # plot cross-correlation\n",
        "        ax = fig.add_subplot(223)\n",
        "        ax.bar(x_corr, cross_corr[t_0_corr:(t_0_corr + 2 * max_lag + 1)])\n",
        "        ax.axhline(y=0, ls='--', c='k')\n",
        "        ax.set_title('Cross Correlation of Energy Difference\\nand Gradient Magnitude', fontsize=fontsize)\n",
        "        ax.set_xlabel('lag', fontsize=fontsize)\n",
        "        ax.set_ylabel('correlation', fontsize=fontsize)\n",
        "        # plot auto-correlation\n",
        "        ax = fig.add_subplot(224)\n",
        "        ax.bar(x_acorr-b_w/2, en_acorr[t_0_acorr:(t_0_acorr + max_lag + 1)], b_w, label='en. diff. $d_{s_t}$')\n",
        "        ax.bar(x_acorr+b_w/2, grad_acorr[t_0_acorr:(t_0_acorr + max_lag + 1)], b_w, label='grad. mag. $r_{s_t}}$')\n",
        "        ax.axhline(y=0, ls='--', c='k')\n",
        "        ax.set_title('Auto-Correlation of Energy Difference\\nand Gradient Magnitude', fontsize=fontsize)\n",
        "        ax.set_xlabel('lag', fontsize=fontsize)\n",
        "        ax.set_ylabel('correlation', fontsize=fontsize)\n",
        "        ax.legend(loc='upper right', fontsize=fontsize-4)\n",
        "\n",
        "    # make diagnostic plots\n",
        "    plot_en_diff_and_grad_mag()\n",
        "    plot_crosscorr_and_autocorr()\n",
        "    # save figure\n",
        "    plt.subplots_adjust(hspace=0.6, wspace=0.6)\n",
        "    plt.savefig(os.path.join(exp_dir, 'diagnosis_plot.pdf'), format='pdf')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "#####################\n",
        "# ## TOY DATASET ## #\n",
        "#####################\n",
        "\n",
        "class ToyDataset:\n",
        "    # TODO: toy_groups I think can be set to 3. toy_sd and toy_radius should be hardcoded?\n",
        "    def __init__(self, toy_type='gmm', toy_groups=8, toy_sd=0.15, toy_radius=1, viz_res=500, kde_bw=0.05):\n",
        "        # import helper functions\n",
        "        from scipy.stats import gaussian_kde\n",
        "        from scipy.stats import multivariate_normal\n",
        "        self.gaussian_kde = gaussian_kde\n",
        "        self.mvn = multivariate_normal\n",
        "\n",
        "        # toy dataset parameters\n",
        "        self.toy_type = toy_type\n",
        "        self.toy_groups = toy_groups\n",
        "        self.toy_sd = toy_sd\n",
        "        self.toy_radius = toy_radius\n",
        "        self.weights = np.ones(toy_groups) / toy_groups\n",
        "        if toy_type == 'gmm':\n",
        "            means_x = np.cos(2*np.pi*np.linspace(0, (toy_groups-1)/toy_groups, toy_groups)).reshape(toy_groups, 1, 1, 1)\n",
        "            means_y = np.sin(2*np.pi*np.linspace(0, (toy_groups-1)/toy_groups, toy_groups)).reshape(toy_groups, 1, 1, 1)\n",
        "            self.means = toy_radius * np.concatenate((means_x, means_y), axis=1)\n",
        "        else:\n",
        "            self.means = None\n",
        "\n",
        "        # ground truth density\n",
        "        if self.toy_type == 'gmm':\n",
        "            def true_density(x):\n",
        "                density = 0\n",
        "                for k in range(toy_groups):\n",
        "                    density += self.weights[k]*self.mvn.pdf(np.array([x[0], x[1]]), mean=self.means[k].squeeze(),\n",
        "                                                            cov=(self.toy_sd**2)*np.eye(2))\n",
        "                return density\n",
        "        elif self.toy_type == 'gmm2': # TODO\n",
        "            def true_density(x):\n",
        "                density = 0\n",
        "                for k in range(toy_groups):\n",
        "                    density += self.weights[k]*self.mvn.pdf(np.array([x[0], x[1]]), mean=self.means[k].squeeze(),\n",
        "                                                            cov=(self.toy_sd**2)*np.eye(2))\n",
        "                return density\n",
        "        elif self.toy_type == 'rings':\n",
        "            def true_density(x):\n",
        "                radius = np.sqrt((x[0] ** 2) + (x[1] ** 2))\n",
        "                density = 0\n",
        "                for k in range(toy_groups):\n",
        "                    density += self.weights[k] * self.mvn.pdf(radius, mean=self.toy_radius * (k + 1),\n",
        "                                                              cov=(self.toy_sd**2))/(2*np.pi*self.toy_radius*(k+1))\n",
        "                return density\n",
        "        else:\n",
        "            raise RuntimeError('Invalid option for toy_type (use \"gmm\", \"gmm2\", or \"rings\")')\n",
        "        self.true_density = true_density\n",
        "\n",
        "        # viz parameters\n",
        "        self.viz_res = viz_res\n",
        "        self.kde_bw = kde_bw\n",
        "        if toy_type == 'rings':\n",
        "            self.plot_val_max = toy_groups * toy_radius + 4 * toy_sd\n",
        "        else:\n",
        "            self.plot_val_max = toy_radius + 4 * toy_sd\n",
        "\n",
        "        # save values for plotting groundtruth landscape\n",
        "        self.xy_plot = np.linspace(-self.plot_val_max, self.plot_val_max, self.viz_res)\n",
        "        self.z_true_density = np.zeros(self.viz_res**2).reshape(self.viz_res, self.viz_res)\n",
        "        for x_ind in range(len(self.xy_plot)):\n",
        "            for y_ind in range(len(self.xy_plot)):\n",
        "                self.z_true_density[x_ind, y_ind] = self.true_density([self.xy_plot[x_ind], self.xy_plot[y_ind]])\n",
        "\n",
        "    def sample_toy_data(self, num_samples):\n",
        "        toy_sample = np.zeros(0).reshape(0, 2, 1, 1)\n",
        "        sample_group_sz = np.random.multinomial(num_samples, self.weights)\n",
        "        if self.toy_type == 'gmm':\n",
        "            for i in range(self.toy_groups):\n",
        "                sample_group = self.means[i] + self.toy_sd * np.random.randn(2*sample_group_sz[i]).reshape(-1, 2, 1, 1)\n",
        "                toy_sample = np.concatenate((toy_sample, sample_group), axis=0)\n",
        "        elif self.toy_type == 'gmm2': # TODO\n",
        "            for i in range(self.toy_groups):\n",
        "                sample_group = self.means[i] + self.toy_sd * np.random.randn(2*sample_group_sz[i]).reshape(-1, 2, 1, 1)\n",
        "                toy_sample = np.concatenate((toy_sample, sample_group), axis=0)\n",
        "        elif self.toy_type == 'rings':\n",
        "            for i in range(self.toy_groups):\n",
        "                sample_radii = self.toy_radius*(i+1) + self.toy_sd * np.random.randn(sample_group_sz[i])\n",
        "                sample_thetas = 2 * np.pi * np.random.random(sample_group_sz[i])\n",
        "                sample_x = sample_radii.reshape(-1, 1) * np.cos(sample_thetas).reshape(-1, 1)\n",
        "                sample_y = sample_radii.reshape(-1, 1) * np.sin(sample_thetas).reshape(-1, 1)\n",
        "                sample_group = np.concatenate((sample_x, sample_y), axis=1)\n",
        "                toy_sample = np.concatenate((toy_sample, sample_group.reshape(-1, 2, 1, 1)), axis=0)\n",
        "        else:\n",
        "            raise RuntimeError('Invalid option for toy_type (\"gmm\", \"gmm2\", or \"rings\")')\n",
        "\n",
        "        return toy_sample\n",
        "\n",
        "    def plot_toy_density(self, plot_truth=False, f=None, epsilon=0.0, x_s_t=None, save_path='toy.pdf'):\n",
        "        num_plots = 0\n",
        "        if plot_truth:\n",
        "            num_plots += 1\n",
        "\n",
        "        # density of learned EBM\n",
        "        if f is not None:\n",
        "            num_plots += 1\n",
        "            xy_plot_torch = t.Tensor(self.xy_plot).view(-1, 1, 1, 1).to(next(f.parameters()).device)\n",
        "            # y values for learned energy landscape of descriptor network\n",
        "            z_learned_energy = np.zeros([self.viz_res, self.viz_res])\n",
        "            for i in range(len(self.xy_plot)):\n",
        "                y_vals = float(self.xy_plot[i]) * t.ones_like(xy_plot_torch)\n",
        "                vals = t.cat((xy_plot_torch, y_vals), 1)\n",
        "                z_learned_energy[i] = f(vals).data.cpu().numpy()\n",
        "            # rescale y values to correspond to the groundtruth temperature\n",
        "            if epsilon > 0:\n",
        "                z_learned_energy *= 2 / (epsilon ** 2)\n",
        "\n",
        "            # transform learned energy into learned density\n",
        "            z_learned_density_unnormalized = np.exp(- z_learned_energy)\n",
        "            bin_area = (self.xy_plot[1] - self.xy_plot[0]) ** 2\n",
        "            z_learned_density = z_learned_density_unnormalized / (bin_area * np.sum(z_learned_density_unnormalized))\n",
        "\n",
        "        # kernel density estimate of shortrun samples\n",
        "        if x_s_t is not None:\n",
        "            num_plots += 1\n",
        "            density_estimate = self.gaussian_kde(x_s_t.squeeze().cpu().numpy().transpose(), bw_method=self.kde_bw)\n",
        "            z_kde_density = np.zeros([self.viz_res, self.viz_res])\n",
        "            for i in range(len(self.xy_plot)):\n",
        "                for j in range(len(self.xy_plot)):\n",
        "                    z_kde_density[i, j] = density_estimate((self.xy_plot[j], self.xy_plot[i]))\n",
        "\n",
        "        # plot results\n",
        "        plot_ind = 0\n",
        "        fig = plt.figure()\n",
        "\n",
        "        # true density\n",
        "        if plot_truth:\n",
        "            plot_ind += 1\n",
        "            ax = fig.add_subplot(2, num_plots, plot_ind)\n",
        "            ax.set_title('True density')\n",
        "            plt.imshow(self.z_true_density, cmap='viridis')\n",
        "            plt.axis('off')\n",
        "            ax = fig.add_subplot(2, num_plots, plot_ind + num_plots)\n",
        "            ax.set_title('True log-density')\n",
        "            plt.imshow(np.log(self.z_true_density + 1e-10), cmap='viridis')\n",
        "            plt.axis('off')\n",
        "        # learned ebm\n",
        "        if f is not None:\n",
        "            plot_ind += 1\n",
        "            ax = fig.add_subplot(2, num_plots, plot_ind)\n",
        "            ax.set_title('EBM density')\n",
        "            plt.imshow(z_learned_density, cmap='viridis')\n",
        "            plt.axis('off')\n",
        "            ax = fig.add_subplot(2, num_plots, plot_ind + num_plots)\n",
        "            ax.set_title('EBM log-density')\n",
        "            plt.imshow(np.log(z_learned_density + 1e-10), cmap='viridis')\n",
        "            plt.axis('off')\n",
        "        # shortrun kde\n",
        "        if x_s_t is not None:\n",
        "            plot_ind += 1\n",
        "            ax = fig.add_subplot(2, num_plots, plot_ind)\n",
        "            ax.set_title('Short-run KDE')\n",
        "            plt.imshow(z_kde_density, cmap='viridis')\n",
        "            plt.axis('off')\n",
        "            ax = fig.add_subplot(2, num_plots, plot_ind + num_plots)\n",
        "            ax.set_title('Short-run log-KDE')\n",
        "            plt.imshow(np.log(z_kde_density + 1e-10), cmap='viridis')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, bbox_inches='tight', format='pdf')\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nets.py\n",
        "\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "#################################\n",
        "# ## TOY NETWORK FOR 2D DATA ## #\n",
        "#################################\n",
        "\n",
        "class ToyNet(nn.Module):\n",
        "    def __init__(self, dim=2, n_f=32, leak=0.05):\n",
        "        super(ToyNet, self).__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Conv2d(dim, n_f, 1, 1, 0), # in, out, kernel size, stride, padding\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Conv2d(n_f, n_f * 2, 1, 1, 0),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Conv2d(n_f * 2, n_f * 2, 1, 1, 0),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Conv2d(n_f * 2, n_f * 2, 1, 1, 0),\n",
        "            nn.LeakyReLU(leak),\n",
        "            nn.Conv2d(n_f * 2, 1, 1, 1, 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.f(x).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_toy.py\n",
        "\n",
        "#############################################\n",
        "# ## TRAIN EBM USING 2D TOY DISTRIBUTION ## #\n",
        "#############################################\n",
        "# Measures and plots the diagnostic values d_{s_t} and r_t\n",
        "\n",
        "# GOAL: Implement another toy density for learning 2D DeepFRAME models.\n",
        "# Modify ToyDataset class in utils to implement another density.\n",
        "# Density should be a GMM groundtruth density with several modes that have\n",
        "# different covariance matrices.\n",
        "# This file (train_toy.py) should run without changes once new dataset class has\n",
        "# been made.\n",
        "\n",
        "import torch as t\n",
        "import json\n",
        "import os\n",
        "\n",
        "# directory for experiment results\n",
        "EXP_DIR = './short_run_demo/out_toy/toy_config_4/'\n",
        "# json file with experiment config\n",
        "CONFIG_FILE = './short_run_demo/config_locker/toy_config.json'\n",
        "\n",
        "\n",
        "#######################\n",
        "# ## INITIAL SETUP ## #\n",
        "#######################\n",
        "\n",
        "# load experiment config\n",
        "with open(CONFIG_FILE) as file:\n",
        "    config = json.load(file)\n",
        "\n",
        "# make directory for saving results\n",
        "if os.path.exists(EXP_DIR):\n",
        "    # prevents overwriting old experiment folders by accident\n",
        "    raise RuntimeError('Experiment folder \"{}\" already exists. Please use a different \"EXP_DIR\".'.format(EXP_DIR))\n",
        "else:\n",
        "    os.makedirs(EXP_DIR)\n",
        "    for folder in ['checkpoints', 'landscape', 'plots', 'code']:\n",
        "        os.mkdir(EXP_DIR + folder)\n",
        "\n",
        "# save copy of code in the experiment folder\n",
        "def save_code():\n",
        "    def save_file(file_name):\n",
        "        file_in = open(file_name, 'r')\n",
        "        file_out = open(EXP_DIR + 'code/' + os.path.basename(file_name), 'w')\n",
        "        for line in file_in:\n",
        "            file_out.write(line)\n",
        "    for file in ['./short_run_demo/train_toy.py', './short_run_demo/nets.py', './short_run_demo/utils.py', CONFIG_FILE]:\n",
        "        save_file(file)\n",
        "save_code()\n",
        "\n",
        "# set seed for cpu and CUDA, get device\n",
        "t.manual_seed(config['seed'])\n",
        "if t.cuda.is_available():\n",
        "    print('t.cuda is available')\n",
        "    t.cuda.manual_seed_all(config['seed'])\n",
        "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "########################\n",
        "# ## TRAINING SETUP # ##\n",
        "########################\n",
        "\n",
        "print('Setting up network and optimizer...')\n",
        "# set up network\n",
        "net_bank = {'toy': ToyNet}\n",
        "f = net_bank[config['net_type']]().to(device)\n",
        "# set up optimizer\n",
        "optim_bank = {'adam': t.optim.Adam, 'sgd': t.optim.SGD}\n",
        "if config['optimizer_type'] == 'sgd' and config['epsilon'] > 0:\n",
        "    # scale learning rate according to langevin noise for invariant tuning\n",
        "    config['lr_init'] *= (config['epsilon'] ** 2) / 2\n",
        "    config['lr_min'] *= (config['epsilon'] ** 2) / 2\n",
        "optim = optim_bank[config['optimizer_type']](f.parameters(), lr=config['lr_init'])\n",
        "\n",
        "print('Processing data...')\n",
        "# toy dataset for which true samples can be obtained\n",
        "q = ToyDataset(config['toy_type'], config['toy_groups'], config['toy_sd'],\n",
        "               config['toy_radius'], config['viz_res'], config['kde_bw'])\n",
        "\n",
        "# initialize persistent states from noise\n",
        "# s_t_0 is used when init_type == 'persistent' in sample_s_t()\n",
        "s_t_0 = 2 * t.rand([config['s_t_0_size'], 2, 1, 1]).to(device) - 1\n",
        "\n",
        "\n",
        "################################\n",
        "# ## FUNCTIONS FOR SAMPLING ## #\n",
        "################################\n",
        "\n",
        "# sample batch from given array of states\n",
        "def sample_state_set(state_set, batch_size=config['batch_size']):\n",
        "    rand_inds = t.randperm(state_set.shape[0])[0:batch_size]\n",
        "    return state_set[rand_inds], rand_inds\n",
        "\n",
        "# sample positive states from toy 2d distribution q\n",
        "def sample_q(batch_size=config['batch_size']): return t.Tensor(q.sample_toy_data(batch_size)).to(device)\n",
        "\n",
        "# initialize and update states with langevin dynamics to obtain samples from finite-step MCMC distribution s_t\n",
        "def sample_s_t(batch_size, L=config['num_mcmc_steps'], init_type=config['init_type'], update_s_t_0=True):\n",
        "    # get initial mcmc states for langevin updates (\"persistent\", \"data\", \"uniform\", or \"gaussian\")\n",
        "    def sample_s_t_0():\n",
        "        if init_type == 'persistent':\n",
        "            return sample_state_set(s_t_0, batch_size)\n",
        "        elif init_type == 'data':\n",
        "            return sample_q(batch_size), None\n",
        "        elif init_type == 'uniform':\n",
        "            return config['noise_init_factor'] * (2 * t.rand([batch_size, 2, 1, 1]) - 1).to(device), None\n",
        "        elif init_type == 'gaussian':\n",
        "            return config['noise_init_factor'] * t.randn([batch_size, 2, 1, 1]).to(device), None\n",
        "        else:\n",
        "            raise RuntimeError('Invalid method for \"init_type\" (use \"persistent\", \"data\", \"uniform\", or \"gaussian\")')\n",
        "\n",
        "    # initialize MCMC samples\n",
        "    x_s_t_0, s_t_0_inds = sample_s_t_0()\n",
        "\n",
        "    # iterative langevin updates of MCMC samples\n",
        "    x_s_t = t.autograd.Variable(x_s_t_0.clone(), requires_grad=True)\n",
        "    r_s_t = t.zeros(1).to(device)  # variable r_s_t (Section 3.2) to record average gradient magnitude\n",
        "    for ell in range(L):\n",
        "        f_prime = t.autograd.grad(f(x_s_t).sum(), [x_s_t])[0]\n",
        "        x_s_t.data += - f_prime + config['epsilon'] * t.randn_like(x_s_t)\n",
        "        r_s_t += f_prime.view(f_prime.shape[0], -1).norm(dim=1).mean()\n",
        "\n",
        "    if init_type == 'persistent' and update_s_t_0:\n",
        "        # update persistent state bank\n",
        "        s_t_0.data[s_t_0_inds] = x_s_t.detach().data.clone()\n",
        "\n",
        "    return x_s_t.detach(), r_s_t.squeeze() / L\n",
        "\n",
        "\n",
        "#######################\n",
        "# ## TRAINING LOOP ## #\n",
        "#######################\n",
        "\n",
        "# containers for diagnostic records (see Section 3)\n",
        "d_s_t_record = t.zeros(config['num_train_iters']).to(device)  # energy difference between positive and negative samples\n",
        "r_s_t_record = t.zeros(config['num_train_iters']).to(device)  # average state gradient magnitude along Langevin path\n",
        "\n",
        "print('Training has started.')\n",
        "for i in range(config['num_train_iters']):\n",
        "    # obtain positive and negative samples\n",
        "    x_q = sample_q()\n",
        "    x_s_t, r_s_t = sample_s_t(batch_size=config['batch_size'])\n",
        "\n",
        "    # calculate ML computational loss d_s_t (Section 3) for data and shortrun samples\n",
        "    d_s_t = f(x_q).mean() - f(x_s_t).mean()\n",
        "    if config['epsilon'] > 0:\n",
        "        # scale loss with the langevin implementation\n",
        "        d_s_t *= 2 / (config['epsilon'] ** 2)\n",
        "    # stochastic gradient ML update for model weights\n",
        "    optim.zero_grad()\n",
        "    d_s_t.backward()\n",
        "    optim.step()\n",
        "\n",
        "    # record diagnostics\n",
        "    d_s_t_record[i] = d_s_t.detach().data\n",
        "    r_s_t_record[i] = r_s_t\n",
        "\n",
        "    # anneal learning rate\n",
        "    for lr_gp in optim.param_groups:\n",
        "        lr_gp['lr'] = max(config['lr_min'], lr_gp['lr'] * config['lr_decay'])\n",
        "\n",
        "    # print and save learning info\n",
        "    if (i + 1) == 1 or (i + 1) % config['log_info_freq'] == 0:\n",
        "        print('{:>6d}   d_s_t={:>14.9f}   r_s_t={:>14.9f}'.format(i+1, d_s_t.detach().data, r_s_t))\n",
        "        # save network weights\n",
        "        t.save(f.state_dict(), EXP_DIR + 'checkpoints/' + 'net_{:>06d}.pth'.format(i+1))\n",
        "        # plot diagnostics for energy difference d_s_t and gradient magnitude r_t\n",
        "        if (i + 1) > 1:\n",
        "            plot_diagnostics(i, d_s_t_record, r_s_t_record, EXP_DIR + 'plots/')\n",
        "\n",
        "    # visualize density and log-density for groundtruth, learned energy, and short-run distributions\n",
        "    if (i + 1) % config['log_viz_freq'] == 0:\n",
        "        print('{:>6}   Visualizing true density, learned density, and short-run KDE.'.format(i+1))\n",
        "        x_kde = sample_s_t(batch_size=config['batch_size_kde'], update_s_t_0=False)[0]\n",
        "        q.plot_toy_density(True, f, config['epsilon'], x_kde, EXP_DIR+'landscape/'+'toy_viz_{:>06d}.pdf'.format(i+1))\n",
        "        print('{:>6}   Visualizations saved.'.format(i + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOTE: Before running train_toy.py code above:\n",
        "\n",
        "1) copy updated code for utils.py and train_toy.py to VSCode, and\n",
        "\n",
        "2) copy updated files (utils.py, train_toy.py, toy_config.json) from VSCode to Google Drive."
      ]
    }
  ]
}